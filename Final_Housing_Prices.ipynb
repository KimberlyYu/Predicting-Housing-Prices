{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Housing Prices.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASGF7Jv4BcK8"
      },
      "source": [
        "Preprocessing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keYBevxQBMW0"
      },
      "source": [
        "import csv\n",
        "import numpy\n",
        "\n",
        "dataFileName = \"AmesHousingSubset.csv\"\n",
        "def load_data():\n",
        "  '''loads the data from the data file into a 2d \n",
        "  list and the feature names into a list and returns both\n",
        "  Also replaces empty values with -1'''\n",
        "  # featureList = []\n",
        "  data = []\n",
        "  with open(dataFileName,\"r\") as fh:\n",
        "    reader = csv.reader(fh)\n",
        "    first = True\n",
        "    x_train = []\n",
        "    x_val = []\n",
        "    y_train = []\n",
        "    y_val = []\n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    y = []\n",
        "    for row in reader:\n",
        "      splitRow = row[2:25]\n",
        "      splitRow.append(row[27]) #6:len(row)-2]\n",
        "      if first:\n",
        "        first = False\n",
        "        # featureList.extend(splitRow)\n",
        "        continue\n",
        "      y.append(row[1])\n",
        "      # Go through row and replace empty values with -1\n",
        "      for i, ele in enumerate(splitRow):\n",
        "        if ele == \"\":\n",
        "          splitRow[i] = \"-1\"\n",
        "      data.append(splitRow)\n",
        "    # print(\"y:\", y)\n",
        "    # split into training and validation\n",
        "    for i, instance in enumerate(data):\n",
        "      if len(x_train) < 800:\n",
        "        x_train.append(instance)\n",
        "        y_train.append(y[i])\n",
        "        continue\n",
        "      if len(x_val) < 300:\n",
        "        x_val.append(instance)\n",
        "        y_val.append(y[i])\n",
        "        continue\n",
        "      x_test.append(instance)\n",
        "      y_test.append(y[i])\n",
        "  print(len(x_test))\n",
        "  return x_train, x_val, y_train, y_val,x_test,y_test\n",
        "\n",
        "def convert_strings_to_floats(data):\n",
        "  '''converts the string values into '''\n",
        "  conversionTable = {\n",
        "  \"Po\": 0.0, \"Fa\" : 0.25, \"TA\" : 0.5, \"Gd\" : 0.75, \"Ex\" : 1.0, \n",
        "  \"FV\" : 0.0, \"RL\" : 0.25, \"RM\" : 0.5, \"RH\" : 0.75, \"C (all)\" : 1.0, \n",
        "  \"Slab\" : 0.0, \"PConc\" : 0.2, \"CBlock\" : 0.4, \"Stone\" : 0.6, \"Wood\" : 0.8, \"BrkTil\" : 1.0, \n",
        "  \"Sev\" : 0.0, \"Maj2\" : 0.167, \"Maj1\" : 0.334, \"Mod\" : 0.501, \"Min2\" : 0.668, \"Min1\" : 0.835, \"Typ\" : 1.0, \n",
        "  \"Y\" : 1.0, \"P\" : 0.5, \"N\" : 0.0\n",
        "  }\n",
        "  for row in range(len(data)):\n",
        "    for col in range(len(data[row])):\n",
        "      if data[row][col] in conversionTable:\n",
        "        # print(\"Before:\",data[row][col])\n",
        "        data[row][col] = conversionTable.get(data[row][col])\n",
        "        # print(\"After:\", data[row][col])\n",
        "  return data\n",
        "\n",
        "def replace_missing_vals(data):\n",
        "  '''replaces all of the -1 values which were once \n",
        "  missing with the mean of the column'''\n",
        "  #print(\"hi\")\n",
        "  #print(len(data))\n",
        "  for column in range(len(data[0])):\n",
        "    rollingSum = 0.0\n",
        "    numVals = 0\n",
        "    for row in range(len(data)):\n",
        "      num = data[row][column]\n",
        "      if not num == -1:\n",
        "        rollingSum += num\n",
        "        numVals += 1\n",
        "    mean = rollingSum/numVals\n",
        "    for row in range(len(data)):\n",
        "      #print(\"Before:\", row)\n",
        "      if(data[row][column] == -1):\n",
        "        data[row][column] = mean\n",
        "      #print(\"After:\", data[row])\n",
        "  return data\n",
        "\n",
        "def scale(data):\n",
        "  '''takes all of the numerical values and \n",
        "  scales them down to a value between 0 and 1'''\n",
        "  for col in range(len(data[1])):\n",
        "    max = 0\n",
        "    for row in range(len(data)):\n",
        "      if(data[row][col]>max):\n",
        "        max = data[row][col]\n",
        "    if max==0:\n",
        "      continue\n",
        "    for row in range(len(data)):\n",
        "      data[row][col] /= max\n",
        "  return data\n",
        "\n",
        "def getCleanData():\n",
        "  # code in here will be run by main.py\n",
        "  x_train, x_val, y_train, y_val, x_test, y_test= load_data()\n",
        "  # print(\"Features:\", feature)\n",
        "  print(\"Testing replacing string with val:\")\n",
        "  x_train = convert_strings_to_floats(x_train)\n",
        "  x_val = convert_strings_to_floats(x_val)\n",
        "  x_test = convert_strings_to_floats(x_test)\n",
        "  print(\"Testing replacing:\")\n",
        "  # print(type(data))\n",
        "  # for row in data:\n",
        "  #   print(len(row))\n",
        "  #   if len(row) == 28:\n",
        "  #     print(row)\n",
        "  x_train = replace_missing_vals([list(map(int, row)) for row in x_train])\n",
        "  x_val = replace_missing_vals([list(map(int, row)) for row in x_val])\n",
        "  x_test = replace_missing_vals([list(map(int, row)) for row in x_test])\n",
        "\n",
        "  print(\"Testing scaling:\")\n",
        "  x_train = scale(x_train)\n",
        "  x_val = scale(x_val)\n",
        "  x_test = scale(x_test)\n",
        "  y_train = list(map(int, y_train))\n",
        "  y_val = list(map(int, y_val))\n",
        "  y_test = list(map(int, y_test))\n",
        "  # print(\"x_train:\", numpy.array(x_train))\n",
        "  # print(\"x_val:\", numpy.array(x_val))\n",
        "  # print(\"y_train:\", numpy.array(y_train))\n",
        "  # print(\"y_val:\", numpy.array(y_val))\n",
        "\n",
        "  print(\"Data is ready for linear regression!!!\")\n",
        "  return x_train, x_val, x_test, y_train, y_val, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuXUbiwWBI8b"
      },
      "source": [
        "Code for linear regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Melf0v6lANea"
      },
      "source": [
        "import numpy\n",
        "\n",
        "cost_history = []\n",
        "accuracy_history = []\n",
        "cost_history_val = []\n",
        "accuracy_history_val = []\n",
        "\n",
        "def r2(pred,y):\n",
        "  sum_y = 0\n",
        "  for i in y:\n",
        "    sum_y+=i\n",
        "  print(sum_y)\n",
        "  ssto = 0\n",
        "  mean_y = sum_y/len(y)\n",
        "  for i in y:\n",
        "    ssto+=(mean_y-i)**2\n",
        "  sse = 0\n",
        "  for i in range(len(pred)):\n",
        "    sse += (pred[i]-y[i])**2\n",
        "  return 1-sse/ssto\n",
        "                     \n",
        "\n",
        "def getPrediction(x, w):\n",
        "  '''h(x) is the prediction based on inputs and weights'''\n",
        "  return float(numpy.sum(numpy.dot(x,w)))\n",
        "\n",
        "def costRow(x,w,y,p):\n",
        "  '''difference between predicted and actual value\n",
        "  '''\n",
        "  pred = getPrediction(x,w)\n",
        "  dif = (pred - y)\n",
        "  # if p:\n",
        "    # print(\"prediction: \"+str(pred)+\" actual: \"+str(y) + \" dif: \" + str(dif))\n",
        "  return dif\n",
        "\n",
        "def costDeriv(x, w, y, learning_rate, feature):\n",
        "  '''finds the derivative for gradient descent\n",
        "  '''\n",
        "  sum = 0\n",
        "  for row in range(len(x)):\n",
        "    sum += costRow(x[row], w, y[row],False) * x[row][feature]\n",
        "  # sum *= learning_rate\n",
        "  sum /= len(x)\n",
        "  return sum\n",
        "\n",
        "def costMat(x,w,y,p):\n",
        "  '''sum squared error\n",
        "  '''\n",
        "  total = 0\n",
        "  totalPercent = 0\n",
        "  for row in range(len(x)):\n",
        "    #total += (costRow(x[row],w,y[row],p)**2)\n",
        "    num = abs(costRow(x[row],w,y[row],p))\n",
        "    total+=num\n",
        "    totalPercent+=num/y[row]\n",
        "  average_error = total/len(x)\n",
        "  # average_cost = numpy.sum(y)/len(y)\n",
        "  accuracy = 100-totalPercent/len(x)*100\n",
        "  return average_error, accuracy # ((average_cost - average_error)/average_cost) * 100\n",
        "\n",
        "def regression(x,y,x_val,y_val,learning_rate=2):\n",
        "  '''Runs multi-variate linear regression algorithm\n",
        "  '''\n",
        "  w = [0 for i in range(len(x[0]))]\n",
        "  iterationNum = 0\n",
        "  while iterationNum < 1250: # 400 times per instance\n",
        "    for feature in range(len(w)): # iterate through each feature in row\n",
        "      jDeriv = costDeriv(x,w,y,learning_rate,feature)\n",
        "      # print(jDeriv)\n",
        "      w[feature] = w[feature] - learning_rate * jDeriv\n",
        "    cost, costPercent = costMat(x,w,y,iterationNum%50==0)\n",
        "    \n",
        "    cost_history.append(cost)\n",
        "    accuracy_history.append(costPercent)\n",
        "    \n",
        "    cost_val, cost_percent_val = costMat(x_val,w,y_val,iterationNum%50==0)\n",
        "    cost_history_val.append(cost_val)\n",
        "    accuracy_history_val.append(cost_percent_val)\n",
        "    \n",
        "    if iterationNum % 10 == 0:\n",
        "      print(\"Iteration #\", iterationNum, \":\", cost, \" percent:\", costPercent)\n",
        "      # print(\"bias:\", w[0])\n",
        "    iterationNum += 1\n",
        "  return w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gGYg_DLAOPj"
      },
      "source": [
        "Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAxyOuauAKvL"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def with_bias(x):\n",
        "  '''Add column of 1s to x for bias'''\n",
        "  for i in range(len(x)):\n",
        "    with_bias = [1.0]\n",
        "    with_bias.extend(x[i])\n",
        "    x[i] = with_bias\n",
        "  return x\n",
        "\n",
        "def plot_error(cost_history):\n",
        "  print(\"cost history:\", cost_history)\n",
        "  plt.plot(cost_history, color = \"red\")\n",
        "  plt.ylabel('Mean error (dollars)')\n",
        "  plt.xlabel('Iterations')\n",
        "  plt.savefig(\"cost-bias.jpg\")\n",
        "  plt.show()\n",
        "\n",
        "def plot_accuracy(accuracy_history):\n",
        "  plt.plot(accuracy_history, color = \"blue\")\n",
        "  plt.ylabel('Relative Accuracy (percent)')\n",
        "  plt.xlabel('Iterations')\n",
        "  plt.savefig(\"accuracy-bias.jpg\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWdqY5ae-0o6"
      },
      "source": [
        "def train(x, y,x_val,y_val):\n",
        "  print(\"hi\")\n",
        "  w = regression(x,y,x_val,y_val)\n",
        "  print(\"done!\")\n",
        "  plot_error(cost_history)\n",
        "  plot_accuracy(accuracy_history)\n",
        "  plot_error(cost_history_val)\n",
        "  plot_accuracy(accuracy_history_val)\n",
        "  print(\"training r2: \", r2(numpy.dot(x,w),y))\n",
        "  return w\n",
        " \n",
        "def test(x,y,w):\n",
        "  #x= with_bias(x)\n",
        "  print(\"average validation:\", costMat(x, w, y, False))\n",
        "#   print(\"average validation:\", r2(numpy.dot(x,w),y))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7h_vqxt695o8"
      },
      "source": [
        "def run_train_and_test():\n",
        "  cost_history.clear()\n",
        "  accuracy_history.clear()\n",
        "  x_train, x_val, x_test, y_train, y_val, y_test = getCleanData()\n",
        "  x_train = with_bias(x_train)\n",
        "  x_test = with_bias(x_test)\n",
        "  x_val = with_bias(x_val)\n",
        "  w = train(x_train, y_train,x_val,y_val)\n",
        "  error, accuracy = costMat(x_test,w,y_test,False)\n",
        "  print(\"Test error: \", error)\n",
        "  print(\"Test accuracy: \", accuracy)\n",
        "  #test(x_val, y_val, w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qVozf9HC8v7",
        "outputId": "f9255fe2-78de-469d-afe5-1660dc037243",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "run_train_and_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "360\n",
            "Testing replacing string with val:\n",
            "Testing replacing:\n",
            "Testing scaling:\n",
            "Data is ready for linear regression!!!\n",
            "hi\n",
            "Iteration # 0 : 64001.709794613096  percent: 57.776973375447575\n",
            "Iteration # 10 : 36415.24260581442  percent: 76.6407985279007\n",
            "Iteration # 20 : 33878.18882454305  percent: 78.40690074240479\n",
            "Iteration # 30 : 33219.03720443806  percent: 78.84326391922365\n",
            "Iteration # 40 : 32837.67980461558  percent: 79.11270829643952\n",
            "Iteration # 50 : 32528.680495370667  percent: 79.31189531006348\n",
            "Iteration # 60 : 32247.980587282153  percent: 79.4807730348138\n",
            "Iteration # 70 : 31985.76495109652  percent: 79.63980644729124\n",
            "Iteration # 80 : 31744.14323182462  percent: 79.78602439489283\n",
            "Iteration # 90 : 31520.531305341803  percent: 79.91959366125843\n",
            "Iteration # 100 : 31318.95955645072  percent: 80.03731324300607\n",
            "Iteration # 110 : 31125.150286130644  percent: 80.14979868346744\n",
            "Iteration # 120 : 30936.871397651586  percent: 80.25913057484145\n",
            "Iteration # 130 : 30753.85302104468  percent: 80.36642797351018\n",
            "Iteration # 140 : 30583.100503885125  percent: 80.46676494916146\n",
            "Iteration # 150 : 30421.82107173989  percent: 80.56199500100908\n",
            "Iteration # 160 : 30270.272672664767  percent: 80.65314191333295\n",
            "Iteration # 170 : 30126.12261152886  percent: 80.74038974242791\n",
            "Iteration # 180 : 29989.495635471874  percent: 80.82414111440525\n",
            "Iteration # 190 : 29856.955858285008  percent: 80.90519278153992\n",
            "Iteration # 200 : 29732.26764555385  percent: 80.97996009111398\n",
            "Iteration # 210 : 29611.689568034028  percent: 81.05165976812987\n",
            "Iteration # 220 : 29500.227069941444  percent: 81.11572803402173\n",
            "Iteration # 230 : 29394.265410897744  percent: 81.1768539682385\n",
            "Iteration # 240 : 29300.78221371383  percent: 81.22391359314352\n",
            "Iteration # 250 : 29210.65234809232  percent: 81.26933951358363\n",
            "Iteration # 260 : 29118.844064471297  percent: 81.31653193842894\n",
            "Iteration # 270 : 29031.07527899538  percent: 81.36134071671341\n",
            "Iteration # 280 : 28942.98894883847  percent: 81.40664310467511\n",
            "Iteration # 290 : 28857.86664530829  percent: 81.45020006398627\n",
            "Iteration # 300 : 28778.241218781684  percent: 81.49047612452516\n",
            "Iteration # 310 : 28704.478978369483  percent: 81.52777485324377\n",
            "Iteration # 320 : 28633.048113434623  percent: 81.56417705452878\n",
            "Iteration # 330 : 28566.23190229059  percent: 81.59743158967831\n",
            "Iteration # 340 : 28503.72925384446  percent: 81.62752165691818\n",
            "Iteration # 350 : 28443.69789279959  percent: 81.65613073629682\n",
            "Iteration # 360 : 28383.865566581084  percent: 81.68553427156513\n",
            "Iteration # 370 : 28331.015590357372  percent: 81.7095388973859\n",
            "Iteration # 380 : 28285.005449099826  percent: 81.72862439278597\n",
            "Iteration # 390 : 28240.251650657676  percent: 81.74745624894794\n",
            "Iteration # 400 : 28195.321573362588  percent: 81.76666209971818\n",
            "Iteration # 410 : 28159.764508937307  percent: 81.78001917246125\n",
            "Iteration # 420 : 28132.162110010417  percent: 81.78687780073955\n",
            "Iteration # 430 : 28110.374847079234  percent: 81.79068867628953\n",
            "Iteration # 440 : 28091.558460442582  percent: 81.7934870586733\n",
            "Iteration # 450 : 28073.50009960979  percent: 81.795661410577\n",
            "Iteration # 460 : 28061.325295277922  percent: 81.79272309483919\n",
            "Iteration # 470 : 28047.769503835927  percent: 81.79148774492089\n",
            "Iteration # 480 : 28030.826918314455  percent: 81.79316132738754\n",
            "Iteration # 490 : 28011.73057898694  percent: 81.79691552034178\n",
            "Iteration # 500 : 27991.29779228521  percent: 81.80236455735935\n",
            "Iteration # 510 : 27971.095141699803  percent: 81.80765251378475\n",
            "Iteration # 520 : 27952.779130925144  percent: 81.81227085363324\n",
            "Iteration # 530 : 27936.769733603775  percent: 81.8155485376748\n",
            "Iteration # 540 : 27918.904931699155  percent: 81.82070384187338\n",
            "Iteration # 550 : 27903.330443637464  percent: 81.82521516899993\n",
            "Iteration # 560 : 27887.977703866243  percent: 81.83073850530054\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6f19b2c08668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_train_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-191b12106186>\u001b[0m in \u001b[0;36mrun_train_and_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwith_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwith_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcostMat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test error: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-44b771a05a6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x, y, x_val, y_val)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mplot_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9ee153e8d45e>\u001b[0m in \u001b[0;36mregression\u001b[0;34m(x, y, x_val, y_val, learning_rate)\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0miterationNum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1250\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# 400 times per instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# iterate through each feature in row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0mjDeriv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcostDeriv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m       \u001b[0;31m# print(jDeriv)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mjDeriv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9ee153e8d45e>\u001b[0m in \u001b[0;36mcostDeriv\u001b[0;34m(x, w, y, learning_rate, feature)\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0msum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0msum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcostRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m   \u001b[0;31m# sum *= learning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0msum\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9ee153e8d45e>\u001b[0m in \u001b[0;36mcostRow\u001b[0;34m(x, w, y, p)\u001b[0m\n\u001b[1;32m     28\u001b[0m   '''difference between predicted and actual value\n\u001b[1;32m     29\u001b[0m   '''\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mdif\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0;31m# if p:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9ee153e8d45e>\u001b[0m in \u001b[0;36mgetPrediction\u001b[0;34m(x, w)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetPrediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;34m'''h(x) is the prediction based on inputs and weights'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcostRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}